# KARMA Configuration: The Mirror Test
# Optimized for Dual-Use Zap Dilemma (Violence vs. Cleaning)

env:
  grid_size: 15          # 15x15 is the "Sweet Spot" for N=6 (Paper Standard)
  num_agents: 6          # High enough density to force interaction
  max_steps: 1000        # Long enough for emergent behavior
  
  # Resource Parameters
  apple_density: 0.65    # Initial patch density
  regrowth_speed: 1.0    # Base speed (controlled by neighbor density)
  waste_spawn_rate: 0.1  # 10% of empty space becomes waste (cleaning task)
  
  # The Dilemma Levers (Crucial!)
  zap_timeout: 50        # Freezing allows monopoly (temptation)
  zap_waste_reward: 0.3  # Cooperative cleaning reward (Medium)
  zap_agent_reward: 0.1  # Violence reward (Low but > 0 temptation)
  zap_cost: 0.01         # Small cost to discourage random firing

training:
  # Experiment Length
  episodes: 10000        # Enough for convergence (approx 8 hours)
  
  # PPO Hyperparameters (Standard MARL)
  lr: 3.0e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_ratio: 0.2
  ppo_epochs: 4
  batch_size: 64         # Batch size for PPO updates
  
  # KARMA Specifics
  contrastive_batch_size: 64
  contrastive_weight: 0.1  # Lambda for KARMA loss
  role_loss_weight: 0.1    # Beta for auxiliary role classification

logging:
  project_name: "karma-mirror-test"
  log_interval: 20       # Log every 20 episodes
  checkpoint_interval: 1000

# tmux new -s karma_run
# #activate venv
# source ~/venv/bin/activate
# cd ~
# rm -rf karma
# git clone https://github.com/tapasiitk/situated-agency-alignment karma
# cd karma
# export PYTHONPATH=.
# pip install -r requirements.txt
# python train_karma.py --config configs/debug_harvest.yaml --mode karma


# # Run all 3 conditions in parallel
# python train_karma.py --config configs/env_harvest.yaml --mode baseline &
# python train_karma.py --config configs/env_harvest.yaml --mode broken &
# python train_karma.py --config configs/env_harvest.yaml --mode karma &

# # After training completes
# python scripts/plot_results.py --wandb-entity <your-entity> --wandb-project karma-mirror-test

# # 1. Deactivate current venv
# deactivate

# # 2. Create new fresh venv
# python3 -m venv venv_karma
# tmux new -s karma_run
# # 3. Activate it
# # Linux/Mac:
# source venv_karma/bin/activate
# # Windows:
# # venv_karma\Scripts\activate

# # 4. Install fresh deps
# pip install --upgrade pip
# git clone https://github.com/tapasiitk/situated-agency-alignment karma
# pip install -r requirements.txt


# Violence Rate
# Fraction of episodes where agents use the "zap" beam to attack others. High values (near 1.0) indicate aggressive defection; ideal KARMA outcome is convergence to ~0.0 as reputational penalties emerge.​

# Cooperation Rate
# Proportion of episodes with mutual non-aggression (no zapping between agents). Fluctuating values suggest exploration; stable high values (>0.8) signal learned social contracts.​

# System Yield
# Total berries harvested across all agents per episode. This captures collective productivity—low yield with high violence confirms "Tragedy of the Commons" baseline.​

# Ethical Selectivity
# KARMA-specific: Measures if agents preferentially aid "good" (cooperative) partners while avoiding/punishing "bad" (violent) ones. 0.5 = random; >0.7 indicates moral learning